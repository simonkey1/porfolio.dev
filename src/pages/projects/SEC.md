---
layout: ../../layouts/MarkdownPostLayout.astro
title: "AnÃ¡lisis en Tiempo Real del Suministro ElÃ©ctrico de Chile"
pubDate: 2025-02-12
description: "Un tutorial de cÃ³mo extraer datos con playwright, procesarlos con pandas y visualizar en Powerbi."
author: "Simon Gomez"
image:
  url: "/Imagen_amarillo1linkedin.png"
  alt: "The Astro logo on a dark background with a pink glow."
tags: ["Powerbi", "Python", "Web Scraping"]
---

## IntroducciÃ³n

Hace poco descubrÃ­ la pÃ¡gina de la Superintendecia de Electricidad y Combustibles (SEC). Me llamÃ³ la atenciÃ³n en particular su secciÃ³n de mÃ©tricas e informaciÃ³n de los cortes de luz a nivel nacional, debido a que notÃ© que hay un gran margen de mejora en su accesibilidad y visualizaciÃ³n de los datos para el usuario.

## Â¿QuÃ© haremos?

1. **Preparar nuestro entorno**: primero, instalaremos las dependencias necesarias para el proyecto.

2. **ExtracciÃ³n de los datos**: aprenderemos a realizar web scraping con Playwright.

3. **TransformaciÃ³n de los datos**: necesitaremos limpiar y procesar los datos extraÃ­dos con Pandas.

4. **VisualizaciÃ³n de los datos**: finalmente, visualizaremos los datos en Powerbi.

## Entorno de Desarrollo

Para este proyecto, necesitaremos instalar las siguientes dependencias: Playwright, Pandas y Powerbi.

Les recomiendo crear un entorno virtual para instalar las dependencias. Para ello, decidan un directorio donde quieran crear el entorno virtual y ejecuten el siguiente comando en la terminal:

```python

        python -m venv "nombre-del-entorno"-env


```

Una vez creado el entorno virtual, actÃ­venlo con el siguiente comando:

```bash

     source "nombre-del-entorno"-env/bin/activate


```

Ahora, instalemos las dependencias necesarias:

```bash

    pip install playwright pandas

```

## ExtracciÃ³n de los Datos

Primero, necesitamos importar las librerÃ­as necesarias:

```python

    from playwright.sync_api import sync_playwright
    import pandas as pd

```

Una vez creado el entorno virtual, actÃ­valo:

bash

source "nombre-del-entorno"-env/bin/activate

# ğŸš€ Instalar Dependencias

Con el entorno virtual activado, instalemos las librerÃ­as necesarias:

```bash
pip install playwright pandas
```

AdemÃ¡s, debemos instalar los navegadores de Playwright:

```bash
playwright install
```

---

## ğŸ” ExtracciÃ³n de Datos con Playwright

Usaremos **Playwright** para interceptar las respuestas de la API de la SEC.

### ğŸ“Œ ImportaciÃ³n de LibrerÃ­as

```python
from playwright.sync_api import sync_playwright
import pandas as pd
import os
import time
import re
from datetime import datetime, timedelta
```

---

## ğŸ“‚ DefiniciÃ³n de Archivos de Salida

Guardaremos los datos en dos archivos CSV:

- **`clientes_afectados_tiempo_real.csv`**: Contiene los datos mÃ¡s recientes.
- **`clientes_afectados_historico.csv`**: Mantiene un registro histÃ³rico.

```python
csv_tiempo_real = "clientes_afectados_tiempo_real.csv"
csv_historico = "clientes_afectados_historico.csv"
```

---

# ğŸ” AnÃ¡lisis de la FunciÃ³n `intercept_responses()`

Esta funciÃ³n usa **Playwright** para interceptar las respuestas de la API de la **Superintendencia de Electricidad y Combustibles de Chile (SEC)** y extraer informaciÃ³n sobre cortes de luz. Luego, almacena estos datos en archivos CSV.

---

## ğŸ“Œ **Estructura General**

1. **Abrir un navegador en modo headless** (sin interfaz grÃ¡fica).
2. **Interceptar las respuestas de la API** en la pÃ¡gina de la SEC.
3. **Extraer informaciÃ³n clave** de los datos JSON recibidos.
4. **Procesar los datos** para calcular tiempos y crear identificadores Ãºnicos.
5. **Guardar la informaciÃ³n en archivos CSV**.
6. **Cerrar el navegador** una vez completado el proceso.

---

## ğŸ›  **Paso a Paso de la FunciÃ³n**

### 1ï¸âƒ£ **Inicializar Playwright y Abrir el Navegador**

Se utiliza `sync_playwright()` para iniciar Playwright y lanzar un navegador **Chromium** en modo **headless** (sin interfaz grÃ¡fica).

```python
with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)  # Lanzar navegador en modo headless
    page = browser.new_page()
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- Crea una instancia de Playwright.
- Lanza un navegador Chromium sin interfaz visual.
- Crea una nueva pÃ¡gina en ese navegador.

---

### 2ï¸âƒ£ **Definir una Lista para Almacenar Registros**

```python
registros = []  # Lista para almacenar datos nuevos
```

ğŸ“Œ **Â¿Para quÃ© sirve?**

- AquÃ­ se guardarÃ¡n los datos extraÃ­dos de la API antes de escribirlos en los archivos CSV.

---

### 3ï¸âƒ£ **Interceptar las Respuestas de la API**

```python
def handle_response(response):
    if "GetPorFecha" in response.url:
        try:
            data = response.json()
            timestamp_actual = datetime.now()  # Tiempo de consulta
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Verifica si la URL de la respuesta contiene `"GetPorFecha"`**, lo que indica que es una respuesta de la API relevante.
- **Convierte la respuesta en JSON** (`response.json()`).
- **Guarda el timestamp actual** para identificar cuÃ¡ndo se hizo la consulta.

---

### 4ï¸âƒ£ **Procesar los Datos ExtraÃ­dos**

```python
for entry in data:
    actualizado_hace = entry.get("ACTUALIZADO_HACE", "")
    minutos_atras = 0

    match = re.search(r'(\d+)\s+Minutos', actualizado_hace)
    if match:
        minutos_atras = int(match.group(1))  # Extrae el nÃºmero antes de "Minutos"

    hora_exacta_reporte = timestamp_actual - timedelta(minutes=minutos_atras)
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Extrae el campo `"ACTUALIZADO_HACE"`**, que indica hace cuÃ¡nto tiempo se actualizÃ³ la informaciÃ³n.
- **Usa una expresiÃ³n regular (`re.search`) para extraer los minutos** mencionados en `"ACTUALIZADO_HACE"`.
- **Calcula la hora exacta del reporte**, restando esos minutos del timestamp actual.

---

### 5ï¸âƒ£ **Crear un Identificador Ãšnico para Cada Registro**

```python
unique_id = f"{entry.get('FECHA_INT_STR', '')}-{entry.get('REGION', '')}-{entry.get('COMUNA', '')}-{entry.get('EMPRESA', '')}-{entry.get('CLIENTES_AFECTADOS', 0)}-{hora_exacta_reporte.strftime('%Y-%m-%d %H:%M:%S')}"
```

ğŸ“Œ **Â¿Por quÃ© es importante esto?**

- **Evita la duplicaciÃ³n de datos**, asegurando que cada registro tenga un ID Ãºnico.
- **Facilita la organizaciÃ³n** en los archivos CSV.

---

### 6ï¸âƒ£ **Guardar los Datos en la Lista `registros`**

```python
registros.append({
    "ID_UNICO": unique_id,
    "TIMESTAMP": timestamp_actual.strftime("%Y-%m-%d %H:%M:%S"),
    "HORA_EXACTA_REPORTE": hora_exacta_reporte.strftime("%Y-%m-%d %H:%M:%S"),
    "FECHA": entry.get("FECHA_INT_STR", ""),
    "REGION": entry.get("NOMBRE_REGION", ""),
    "COMUNA": entry.get("NOMBRE_COMUNA", ""),
    "EMPRESA": entry.get("NOMBRE_EMPRESA", ""),
    "CLIENTES_AFECTADOS": entry.get("CLIENTES_AFECTADOS", 0),
    "ACTUALIZADO_HACE": actualizado_hace
})
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Guarda cada registro como un diccionario** dentro de la lista `registros`.
- **Almacena los datos clave** como fecha, regiÃ³n, comuna, empresa y clientes afectados.

---

### 7ï¸âƒ£ **Capturar las Respuestas de la API**

```python
page.on("response", handle_response)
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Asocia la funciÃ³n `handle_response` con cada respuesta de la pÃ¡gina**.
- **Intercepta las respuestas en segundo plano** mientras se carga la web.

---

### 8ï¸âƒ£ **Acceder a la PÃ¡gina de la SEC**

```python
page.goto("https://apps.sec.cl/INTONLINEv1/index.aspx")
page.wait_for_timeout(5000)  # Espera para capturar datos
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Abre la pÃ¡gina de la SEC en el navegador**.
- **Espera 5 segundos** para permitir la carga de datos.

---

### 9ï¸âƒ£ **Cerrar el Navegador**

```python
browser.close()
```

ğŸ“Œ **Â¿Por quÃ© es importante?**

- **Libera recursos del sistema**.
- **Evita que el script consuma demasiada memoria**.

---

## ğŸ“Š **Guardado de Datos en CSV**

```python
if registros:
    df_new = pd.DataFrame(registros)

    # ğŸ“Œ Guardar en CSV histÃ³rico
    if os.path.exists(csv_historico):
        df_historico = pd.read_csv(csv_historico, encoding="utf-8-sig")
        df_historico = pd.concat([df_historico, df_new]).drop_duplicates(subset=["ID_UNICO"], keep="first")
    else:
        df_historico = df_new

    df_historico.to_csv(csv_historico, index=False, encoding="utf-8-sig")

    # ğŸ“Œ Guardar en CSV de Tiempo Real
    df_new.to_csv(csv_tiempo_real, index=False, encoding="utf-8-sig")

    print(f"âœ… Datos guardados en:\nğŸ“Œ {csv_historico} (HistÃ³rico)\nğŸ“Œ {csv_tiempo_real} (Tiempo Real)")
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Convierte los registros en un DataFrame de Pandas**.
- **Guarda los datos en CSV histÃ³rico y de tiempo real**.
- **Evita duplicados basÃ¡ndose en el ID Ãºnico**.

---

## ğŸ” **AutomatizaciÃ³n Cada 5 Minutos**

```python
while True:
    intercept_responses()
    print("â³ Esperando 5 minutos para la siguiente ejecuciÃ³n...\n")
    time.sleep(5 * 60)  # 5 minutos en segundos
```

ğŸ“Œ **Â¿QuÃ© hace esto?**

- **Ejecuta `intercept_responses()` en un bucle infinito**.
- **Espera 5 minutos (`time.sleep(5 * 60)`) antes de volver a ejecutar la funciÃ³n**.

---

âœ… **Â¡Con esto, el script puede monitorear los cortes de luz en tiempo real y guardarlos en CSV de manera automÃ¡tica!** ğŸš€
